{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open chrome for web scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "def search_resume(title, location):\n",
    "    head = 'https://www.indeed.com/resumes?rsrdr=1&hl=en&co=US'\n",
    "    driver.get(head)\n",
    "    search_form = driver.find_element_by_id(\"query\")\n",
    "    search_form.send_keys(title)\n",
    "    search_form = driver.find_element_by_id(\"location\")\n",
    "    search_form.clear()\n",
    "    search_form.send_keys(location)\n",
    "    search_form.send_keys(Keys.RETURN)\n",
    "    \n",
    "def get_links(soup):\n",
    "    #get all resume links on a page\n",
    "    #soup = BeautifulSoup('page',lxml)\n",
    "    items = soup.find_all('div', class_='app_name')\n",
    "    links = []\n",
    "    for item in items:\n",
    "        link = item.find('a')\n",
    "        if link:\n",
    "            link = 'https://www.indeed.com' + link['href'] + '\\n'\n",
    "            links.append(link)\n",
    "    return ''.join(links)\n",
    "\n",
    "def save_links(filename):\n",
    "    f = open(filename, 'a')\n",
    "    c = 0\n",
    "\n",
    "    while True:\n",
    "        page = driver.page_source\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "    \n",
    "        f.write(get_links(soup))\n",
    "        time.sleep(5+random.uniform(0.0,2.0))\n",
    "        if soup.find('a', class_='confirm-nav next'):\n",
    "            sys.stdout.write(' P' + str(c))\n",
    "            c += 1\n",
    "            nextpage= driver.find_element_by_xpath('//*[@class=\"confirm-nav next\"]')\n",
    "            nextpage.click()\n",
    "        else:\n",
    "            break\n",
    "    f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'DC']\n"
     ]
    }
   ],
   "source": [
    "states = []\n",
    "file = open('c:\\python\\states.dat', 'r')\n",
    "for line in file:\n",
    "    states.append(line[:2])\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P0 P1"
     ]
    }
   ],
   "source": [
    "chromedriver = 'c:/chromedriver'\n",
    "os.environ['webdriver.chrome.driver'] = chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "for state in states[0:4]:\n",
    "    search_resume('data scientist', state)   \n",
    "    save_links('test1.dat')\n",
    "    time.sleep(30+random.uniform(0,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
