{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class resume(object):\n",
    "    \n",
    "    def __init__(self, driver, link):\n",
    "        self.driver = driver\n",
    "        self.link = link\n",
    "        self.driver.get(self.link)\n",
    "        \n",
    "        self.soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "        self.resume_information()\n",
    "    \n",
    "    def resume_information(self):\n",
    "        self.information = {}\n",
    "        self.information['index'] = self.index()\n",
    "        self.information['title'] = self.title()\n",
    "        self.information['headline'] = self.headline()\n",
    "        self.information['location'] = self.locality()\n",
    "        #information['summary'] = self.summary()\n",
    "        self.information['work_experience'] = self.experience()\n",
    "        self.information['education'] = self.education()\n",
    "        self.information['skills'] = self.skills()\n",
    "        self.information['addtional_information'] =self.additional()\n",
    "    \n",
    "    def index(self):\n",
    "        try:\n",
    "            return self.soup.find('meta',{'name':'resumeId'})['content']\n",
    "        except:\n",
    "            return 'NA'\n",
    "        \n",
    "    def title(self):\n",
    "        try:\n",
    "            return self.soup.find('h1', id='resume-contact').text.strip(' \\n')\n",
    "        except:\n",
    "            return 'NA'\n",
    "    def headline(self):\n",
    "        try:\n",
    "            return self.soup.find('h2', id='headline').text\n",
    "        except:\n",
    "            return 'NA'\n",
    "    def locality(self):\n",
    "        try:\n",
    "            return self.soup.find('p', id='headline_location').text\n",
    "        except:\n",
    "            return 'NA'\n",
    "    def skills(self):\n",
    "        try:\n",
    "            all_skills = self.soup.find('div', class_='skill-container resume-element').text\n",
    "            return [skill for skill in all_skills.split(', ')]\n",
    "        except:\n",
    "            return 'NA'\n",
    "    def experience(self):\n",
    "        try:\n",
    "            #exps_rep = re.complie('workExperience-\\w+')\n",
    "            \n",
    "            exps_list = self.soup.find_all('div', class_='work-experience-section')\n",
    "            exps = []\n",
    "            for exp in exps_list:\n",
    "                dict1 = {}\n",
    "                dict1['title'] = exp.find('p', class_='work_title title').text\n",
    "                dict1['company'] = exp.find('div', class_='work_company').text\n",
    "                dict1['dates'] = exp.find('p', class_='work_dates').text\n",
    "                dict1['description'] = exp.find('p', class_='work_description').text\n",
    "                exps.append(dict1)\n",
    "            return exps\n",
    "        except:\n",
    "            return 'NA'\n",
    "    def education(self):\n",
    "        try:\n",
    "            edus_list = self.soup.find_all('div', class_='education-section')\n",
    "            edus = []\n",
    "            for edu in edus_list:\n",
    "                dict1 = {}\n",
    "                dict1['title'] = edu.find('p', class_='edu_title').text\n",
    "                dict1['school'] = edu.find('div', class_='edu_school').text\n",
    "                dict1['dates'] = edu.find('p', class_='edu_dates').text\n",
    "                edus.append(dict1)\n",
    "            return edus\n",
    "        except:\n",
    "            return 'NA'\n",
    "    def additional(self):\n",
    "        try:\n",
    "            return self.soup.find('div', id='additionalinfo-section').text\n",
    "        except:\n",
    "            return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--dns-prefetch-disable')\n",
    "chromedriver = 'c:\\chromedriver'\n",
    "os.environ['webdriver.chrome.driver'] = chromedriver\n",
    "driver = webdriver.Chrome(chromedriver,chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    print(len(lines))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def obtain_resume(lines):\n",
    "    for i in range(250,len(lines)):\n",
    "        link = lines[i]\n",
    "        if i%10 == 0:\n",
    "            time.sleep(30+random.uniform(0,3))\n",
    "        else:\n",
    "            time.sleep(3+random.uniform(0,3))\n",
    "        resume_r = resume(driver,link)\n",
    "        data = resume_r.information\n",
    "\n",
    "        with open(data['index']+'.json', 'w') as fp:\n",
    "            json.dump(data, fp)\n",
    "\n",
    "        sys.stdout.write('L' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n",
      "L250L251L252L253L254L255L256L257L258L259L260L261L262L263L264L265L266L267L268L269L270L271L272L273L274L275L276L277L278L279L280L281L282L283L284L285L286L287L288L289L290L291L292L293L294L295L296L297L298L299L300L301L302L303L304L305L306L307L308L309L310L311L312L313L314L315L316L317L318L319L320L321L322L323L324L325L326L327L328L329L330L331L332L333L334L335L336L337L338L339L340L341L342L343L344L345L346L347L348L349L350L351L352L353L354L355L356L357L358L359L360L361L362L363L364L365L366L367L368L369L370L371L372L373L374L375L376L377L378L379L380L381L382L383L384L385L386L387L388L389L390L391L392L393L394L395L396L397L398L399L400L401L402L403L404L405L406L407L408L409L410L411L412L413L414L415L416L417L418L419L420L421L422L423L424L425L426L427L428L429L430L431L432L433L434L435L436L437L438L439L440L441L442L443L444L445L446L447L448L449L450L451L452L453L454L455L456L457L458L459L460L461L462L463L464L465L466L467L468L469L470L471L472L473L474L475L476L477L478L479L480L481L482L483L484"
     ]
    }
   ],
   "source": [
    "lines = read_lines('indeed_links_DS_IL.dat')\n",
    "obtain_resume(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
